{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many situation where we may want to compare two probability distributions.\n",
    "\n",
    "Specifically, we may have a single random variable and two different probability distributions for the variables, such as a true distribution and an approxomationof that distribution.\n",
    "\n",
    "In situation like this, it can be useful to quantify the difference between the distributions.Generally, this is referred to as the problem of calculating the statistical distance between two statistical objects,e.g. probability distributions.\n",
    "\n",
    "One approch is to calculate a distance measure between the two distributions.This can be challenging as it can be difficult to interpret the measure.\n",
    "\n",
    "Instead, it is more common to calculate a divergence between two probability distributions.A divergence is like a measure but is not symmetrical.This means that a divergence is a scoring of how one distribution differs from another,where calculating the divergence for distributions P and Q would give a different score from Q and P.\n",
    "\n",
    "Divergence scores are an imoortant foundation for many diferent calculations in information theory and more generally in Machine Learning.For example, they provide shortuts for calculating scores such as \"mutual information(information gain)\" and cross-entropy used as a loss function for classification models.\n",
    "\n",
    "Divergence scores are also used directly as tools for understanding complex modeling problems, such as approximating a target probability distribution when optimizing GAN(Generative Adversarial Network).\n",
    "\n",
    "Two commonly used divergence scores from information theory are Killback-Leibler Divergence and Jensen-Shannon Divergence."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Statistical distance is the general idea of calculating the different between statistical objects like different probability \n",
    "\n",
    "distributions for a random variable."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Kullback-Leibler divergence calculates a score that measures the divergence of one probability distribution from another. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Jensen-Shannon divergence extends KL divergence to calculate a symmetrical score and distance measure of one probaility\n",
    "\n",
    "distribution from another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Kullback-Leibler Divergence score, or KL divergence score, quantifies how much onr probability distribution differs from another probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL(P || Q) = KL divergence between two distribution P and Q.\n",
    "\n",
    "In KL(P || Q) , \"||\" operator indicates \"divergence\" or Ps divergence from Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL divergence can be calculated as the negative sum of probability of each event in P multiplied by the log of the probability of the event in Q over the probability of the event in P."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL(P || Q) = - summation over x in xP(x) * log(P(x)/Q(x))\n",
    "\n",
    "                        or\n",
    "\n",
    "KL(P || Q) = summation over x in xP(x) * log(Q(x)/P(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition for the KL divergence score is that when the probability for an event from P is large, but the probability for the same event in Q is small, there is a large divergence.When the probability from P is small and the probability from Q is large,there is also a large divergence, but not as large as the first case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be used to measure the divergence between discrete and contiuous probability distributions, where in the latter case the integral of the events is calculated instead of the sum of the probabilities of discrete events."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KL divergence is not symmetrical\n",
    "\n",
    "\n",
    "i.e. KL(P || Q)  != KL(Q || P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = %.3f q = %.3f (1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# define distributions\n",
    "events = ['red','green','blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "print(\"p = %.3f q = %.3f\", (sum(p), sum(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot first distribution\n",
    "plt.subplot(2,1,1)\n",
    "plt.bar(events, p)\n",
    "# plot second distribution\n",
    "plt.subplot(2,1,2)\n",
    "plt.bar(events,q)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the kl divegence\n",
    "def kl_divergence(p, q):\n",
    "    return sum(np.array([p[i] * math.log2(p[i]/q[i]) for i in range(len(p))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Here we using log base 2, give units in 'bits'.\n",
    "\n",
    "2. Here we using log base e, give units in 'nats'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P || Q): 1.927 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate (P || Q)\n",
    "kl_pq = kl_divergence(p,q)\n",
    "print(\"KL(P || Q): %.3f bits\"% kl_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above case,the divergence of P from Q is just under 2 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(Q || P): 2.022 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate (Q || P)\n",
    "kl_qp = kl_divergence(q,p)\n",
    "print(\"KL(Q || P): %.3f bits\"% kl_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above case,the divergence of Q from P as just over 2 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use rel_entr() function, from scipy libraries, it takes log base \"e\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = %.3f q = %.3f (1.0, 1.0)\n",
      "KL(P || Q): 1.336 bits\n",
      "KL(Q || P): 1.401 bits\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import rel_entr\n",
    "# define distributions\n",
    "events = ['red','green','blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "print(\"p = %.3f q = %.3f\", (sum(p), sum(q)))\n",
    "# calculate (P || Q)\n",
    "kl_pq = rel_entr(p,q)\n",
    "print(\"KL(P || Q): %.3f bits\"% sum(kl_pq))\n",
    "# calculate (Q || P)\n",
    "kl_qp = rel_entr(q,p)\n",
    "print(\"KL(Q || P): %.3f bits\"% sum(kl_qp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is intutive if we consider P has large probabilities when Q is small, giveing P less divergence than Q from P as Q has more small probabilities when P has large probabilities.There is more divergence in this second case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JS Divergence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "JS is Symmetrical\n",
    "\n",
    "JS(P || Q) == JS(Q || P)\n",
    "\n",
    "Formula: \n",
    "    \n",
    "JS(P || Q) = (1/2) * KL(P || M) + (1/2) * KL(Q || M)\n",
    "\n",
    "M = (1/2) * (P + Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JS divergence is more useful as a measure as it provides a smoothed and normalized version of KL divergence, with scores between 0 (identical) and 1 (maximally different), when using the base-2 logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square root of the score gives a quantity referred to as the JS-distance or JS-distance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate KL divergence\n",
    "def kl_divergence(p,q):\n",
    "    return sum([p[i] * math.log2(p[i]/q[i]) for i in range(len(p))])\n",
    "# claculate JS divergence\n",
    "def js_divergence(p,q):\n",
    "    m = [0.5 * (p[i] + q[i]) for i in range(len(p))]\n",
    "    return 0.5 * (kl_divergence(p,m) + kl_divergence(q,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = %.3f q = %.3f (1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# define distributions\n",
    "events = ['red','green','blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "print(\"p = %.3f q = %.3f\", (sum(p), sum(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS(P || Q) divergence: 0.420 bits\n",
      "JS(P || Q) distance: 0.648\n"
     ]
    }
   ],
   "source": [
    "# calculate JS(P || Q)\n",
    "js_pq = js_divergence(p,q)\n",
    "print('JS(P || Q) divergence: %.3f bits'%js_pq)\n",
    "print('JS(P || Q) distance: %.3f'%math.sqrt(js_pq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS(Q || P) divergence: 0.420 bits\n",
      "JS(Q || P) distance: 0.648\n"
     ]
    }
   ],
   "source": [
    "# calculate JS(Q || P)\n",
    "js_qp = js_divergence(q,p)\n",
    "print('JS(Q || P) divergence: %.3f bits'%js_qp)\n",
    "print('JS(Q || P) distance: %.3f'%math.sqrt(js_qp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = %.3f q = %.3f (1.0, 1.0)\n",
      "JS(P || Q) divergence: 0.420 bits\n",
      "JS(P || Q) distance: 0.648\n",
      "JS(Q || P) divergence: 0.420 bits\n",
      "JS(Q || P) distance: 0.648\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "# define distributions\n",
    "events = ['red','green','blue']\n",
    "p = np.array([0.10, 0.40, 0.50])\n",
    "q = np.array([0.80, 0.15, 0.05])\n",
    "print(\"p = %.3f q = %.3f\", (sum(p), sum(q)))\n",
    "# calculate JS(P || Q)\n",
    "js_pq = jensenshannon(p,q, base = 2)\n",
    "print('JS(P || Q) divergence: %.3f bits'%(js_pq**2))\n",
    "print('JS(P || Q) distance: %.3f'%js_pq)\n",
    "# calculate JS(Q || P)\n",
    "js_qp = jensenshannon(q,p, base = 2)\n",
    "print('JS(Q || P) divergence: %.3f bits'%(js_qp**2))\n",
    "print('JS(Q || P) distance: %.3f'%js_qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = %.3f q = %.3f (1.0, 1.0)\n",
      "JS(P || Q) divergence: 0.291 bits\n",
      "JS(P || Q) distance: 0.540\n",
      "JS(Q || P) divergence: 0.291 bits\n",
      "JS(Q || P) distance: 0.540\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "# define distributions\n",
    "events = ['red','green','blue']\n",
    "p = np.array([0.10, 0.40, 0.50])\n",
    "q = np.array([0.80, 0.15, 0.05])\n",
    "print(\"p = %.3f q = %.3f\", (sum(p), sum(q)))\n",
    "# calculate JS(P || Q)\n",
    "js_pq = jensenshannon(p,q)\n",
    "print('JS(P || Q) divergence: %.3f bits'%(js_pq**2))\n",
    "print('JS(P || Q) distance: %.3f'%js_pq)\n",
    "# calculate JS(Q || P)\n",
    "js_qp = jensenshannon(q,p)\n",
    "print('JS(Q || P) divergence: %.3f bits'%(js_qp**2))\n",
    "print('JS(Q || P) distance: %.3f'%js_qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
